---
title: "Choose your own splitting criteria"
subtitle: "An adventure with random forests"
author: "Andee Kaplan"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: true
    number_sections: true
bibliography: biblio.bib
nocite: |
  @devtools
vignette: >
  %\VignetteIndexEntry{Choose your own splitting criteria}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

#Background

The random forest [@breiman2001random] is an ensemble method that combines bagged (bootstap aggregated) classification and regression trees with a random variable selection at each node in the tree. This methodology is implemented in `R` [@R] in the `randomForest` package [@randomForest]. The random forest methodology is a general framework for building a model by combining trees, however its implementation is very specific. Within the `randomForest` package, a Gini splitting criteria is used as the only possible splitting mechanism within the trees.

The `forestr` package [@forestr] implements the random forest methodology with a choice of splitting criteria, namely:

  - Gini,
  - Information,
  - One-sided Extremes, and
  - One-sided Purity.
  
The last two splitting criteria are introduced by Buja and Lee [-@buja2001data] as a means to "identify pure or extreme buckets" in the data.

#Why

You may think to yourself, "Why would I need this? The `randomForest` package works well and is blazing fast!" I would agree. If Gini is your splitting criteria of choice and your data are fairly balanced, then I would recommend using `randomForest`. If, however, your data is not evenly spread across the classification buckets or there is a particular class of data that you care more about classifying correctly (especially if it is a small minority of cases), then you may turn to a one-sided extremes or one-sided purity splitting function. This is the case often with detection problems, for example cancer detection in pations or detection of the Higgs particle from measurements by particle detectors in an accelerator.

#Higgs Example


#Implementation

The `forestr` package is written entirely in `R` and wraps the random forest methodology around a modified `rpart` package [@rpart] to include the additional splitting criteria. The modified `rpart` can be found at http://github.com/andeek/rpart and installed using the command 

```r
devtools::install_github("andeek/rpart")
```

The `rpart` functionality is used to create stub trees (trees of maximum depth one) from a random selection of variables on the bootstrapped data at each eligible node. A detailed diagram of how this works can be found in the figure below. The dataset is used to create $B$ bootstrap sampled datasets which are then used to create $B$ trees. Each tree is created by recursively splitting each terminal node on a set of randomly sampled $m$ variables from the dataset until this process can continue no longer. `forestr` uses `rpart` to split each terminal node at most depth of one.

<img src="images/forest_rpart.png" width = 100%/>


After the $B$ trees are created, there must be a mechanism by which one can predict new data classification (or regression values) from the created "forest" trees. Since they are no longer `rpart` trees, there is no built in method to accomplish this. The solution is to use the path generated by `rpart` as a means to filter the new dataset into predicted nodes in the forest tree and assign those observations the value predicted by the forest tree. After predicting the forest tree value for each observation of new data, the predictions are combined via a majority voting method for classification or a bootstrap average for regression trees, which follows the standard random forest method.

The modifications to `rpart` for the additional splitting functions were first introduced in the package `itree` [@itree], however this package was build on an old and slightly unstable version of `rpart`, making it unusable as is within the `forestr` framework. Fortunately, `rpart` allows for the addition of user defined splitting criterion using either `C` functions or `R` functions written by the user. The three functions a user must define are

init

:   The initialization function that initializes the parameters and data passed to the splitting function

eval

:   The evaluation function that creates a label for each node and evaluated the deviance at that node

split
    
:   The splitting function is the workhorse function that actually attempts to find the split point for the data based on a variable

As such, the modified `rpart` contains the additional `C` splitting functions provided from the `itree` package, but within the more stable current release of `rpart`, version 4.1-9. 

By using `rpart` to create the stubs in `forestr`, this allows `forestr` to be a truly flexible method for implementing the random forest methodology. If desired, a user can create her own splitting function by following the framework defined in `rpart` and extending the random forest methodology to adapt to a new splitting mechanism. For details on creating a user defined splitting mechanism, please see the [vignette](http://cran.r-project.org/web/packages/rpart/vignettes/usercode.pdf) that describes the process in `rpart`.

#References
